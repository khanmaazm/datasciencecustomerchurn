{"cells":[{"cell_type":"code","source":["from unidecode import unidecode\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nfrom datetime import datetime"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"952a5b01-9b1e-40dc-ac74-9a55ba9dedae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def format_as_character(x):\n    \"\"\"\n    Formats string list in a standarized format: removing multiple spaces and non alphanumeric characters, standarizing words to not have accents and making all letters lowercase\n    \n    Parameters:\n    x: string list to format\n    \n    Returns:\n    String list formatted\n    \"\"\"\n    # remove accents and make lowercase\n    x = [unidecode(str(x)).lower() for x in x]\n    # remove leading and tailing white spaces\n    x = pd.Series(x).str.strip()\n    # replace multiple spaces with one\n    x = pd.Series([re.sub(r\"\\s+\", \" \", x) for x in x])\n    # remove non alphanumeric characters\n    x = pd.Series([re.sub(r\"[^a-zA-Z0-9_ ]\", \" \", x) for x in x])\n    # replace yet again multiple spaces with one\n    x = pd.Series([re.sub(r\"\\s+\", \" \", x) for x in x])\n    # remove leading and tailing white spaces yet again\n    x = x.str.strip()\n    # replace spaces with underscores\n    x = pd.Series([re.sub(\" \", \"_\", x) for x in x])\n    # replace string nans with numpy nans\n    x = [x if x != 'nan' else np.NaN for x in list(x)]\n\n    return x\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be41d5fe-3391-4909-900c-c08533dcc800"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def list_files(path, full_path = False):\n  \n    \"\"\"\n\n    Returns list of files in the input path. \n\n    Parameters:\n    path: path for which to get list of files.\n    full_path: flag indicating whether to return the full path.\n\n    Returns:\n    List of files on path. \n\n    \"\"\"\n  \n    if full_path:\n        file_names = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n    else:\n        file_names = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n    # make sure to remove temporary files\n    file_names = [x for x in file_names if not x.startswith('~$')]\n\n    return file_names"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"038cc519-09da-45f6-a941-01f4e2f38c87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_output_file_name(config, file_name, file_dir = None, is_model = False):\n  \n    \"\"\"\n\n    Generates the full path for an output file. It creates file_dir if it does not exist in the config dictionary, and adds a time stamp before the file name. If the output is a model, the timestamp includes hours minutes and seconds, and is restricted to the day otherwise.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    file_name: base file name.\n    file_dir: name of the folder in the outputs folder in which to save the output.\n    is_model: indicates whether the output is from modelling\n\n    Returns:\n    File name of the form {file_dir}/{time_stamp}_{file_name}\n\n    \"\"\"\n    \n    if is_model:\n        assert file_dir is None\n        full_path = config['paths']['models_folder']\n        if not os.path.isdir(full_path):\n          os.makedirs(full_path) \n    else:\n        full_path = config['paths']['output']\n\n    if file_dir is not None:\n        full_path = os.path.join(full_path, file_dir)\n        if not os.path.isdir(full_path):\n            os.makedirs(full_path)\n\n    if is_model:\n        full_path = os.path.join(full_path, '{}_{}'.format(datetime.today().strftime('%Y%m%d_%H%M%S'), file_name))\n    else:\n        full_path = os.path.join(full_path, '{}_{}'.format(datetime.today().strftime('%Y%m%d'), file_name))\n\n    return full_path   "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45a84972-7670-41bb-9848-d0421bcbfd38"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def save_dict_to_excel(dict_to_save, config, output_file = None, output_folder = None, full_file_name = None, index = False, is_model = False):\n  \n    \"\"\"\n\n    Saves a dictionary of data frames into one excel with several sheets. The keys of the dictionary define the names of the sheets. The output path is obtained throgh get_output_file_name\n\n    Parameters:\n    dict_to_save: dictionary of data frames to be saved.\n    output_file: base file name.\n    output_folder: name of the folder in the outputs folder in which to save the output.\n    config: config dictionary as defined in get_config.\n    index: indicates whether the to include the index in the excel sheets.\n\n    Returns:\n    Nothing\n\n    \"\"\"  \n  \n    if full_file_name is None:\n      full_file_name = get_output_file_name(config, output_file, output_folder, is_model)\n    writer = pd.ExcelWriter(full_file_name, engine='xlsxwriter')\n    for i in dict_to_save.keys():\n        dict_to_save[i].to_excel(writer, sheet_name = i, index = index)\n    writer.save()\n    return None"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"658a00ef-8147-405f-b161-3124ac119ea6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_columns_dict(config, table_name):\n\n    \"\"\"\n\n    Extracts information on a specific dataset from the config dictionary.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    table_name: table name for which to extract the information.\n\n    Returns:\n    Dictionary with the columns to drop, to keep, to rename, the id columns, the NA replacements to perform and the column formats as saved in the config dictionary.\n\n    \"\"\"\n  \n    columns_dict = {\n        'to_drop': get_table_info(config, table_name, 'cols_to_drop'),\n        'to_keep': get_table_info(config, table_name, 'cols_to_keep'),\n        'new_names': get_table_info(config, table_name, 'renaming_cols'),\n        'id_cols': get_table_info(config, table_name, 'id_cols'),\n        'na_replacements': get_table_info(config, table_name, 'fill_nas'),\n        'formats': get_table_formats(config, table_name)\n    }\n\n    return columns_dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83523ff2-e21f-4706-a040-fdea97fc9cb5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_table_formats(config, table_name):\n  \n    \"\"\"\n\n    Extracts column formats for a specific dataset from the config dictionary.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    table_name: table name for which to extract the information.\n\n    Returns:\n    Dictionary with the different table formats to assign.\n\n    \"\"\"\n    \n    \n    table_formats = get_table_info(config, table_name, 'column_formats')\n\n    if table_formats is not None:\n        formats_declared = list(table_formats.keys())\n\n        columns_declared = []\n        for i in formats_declared:\n            if i == 'dummify':\n                pass\n            else:\n                columns_declared.extend(table_formats[i])\n\n        assert len(columns_declared) == len(\n            set(columns_declared)), 'There are columns duplicated in the format section for {}'.format(table_name)\n\n        allowed_formats = ['numeric', 'character', 'dummify', 'forced_numeric', 'id']\n        assert all([i in allowed_formats for i in\n                    formats_declared]), 'The get_table_formats only accepts numeric, forced numeric, character, id, and dummy formats'\n\n    return table_formats"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a3af0c8-5eab-447d-a49e-8e874073f1a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_table_info(config, table_name, info_label):\n  \n    \"\"\"\n\n    Extracts a specific piece of information (e.g., columns to drop) from the params specified for the data set in the config dictionary\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    table_name: table name for which to extract the information.\n    info_label: information to extract.\n\n    Returns:\n    Information for one action (e.g., drop columns) specified for the inputted data set in the config dictionary\n\n    \"\"\"  \n  \n    assert table_name in config['data_params'].keys(), 'The table specified is not in the configuration file'\n    if info_label in config['data_params'][table_name].keys():\n        return config['data_params'][table_name][info_label]\n    else:\n        return None\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6de7a296-2fe3-43d3-bbb2-5898c100e25f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def clean_df(df, config, table_name, columns_dict=None, check_column_existance=True, verbose=False):\n  \n    \"\"\"\n\n    Performs several cleaning actions on the inputted data set, and returns the cleaned version. This actions are:\n\n    - Using format_as_character to format the column names.\n    - Rename the columns as specified in the config file.\n    - Drop columns as specified in the config file.\n    - Replaces NAs as specified in the config file.\n    - Keeps columns as specified in the config file.\n    - Formatting columns in different formats:\n        - character (using format_as_character)\n        - numeric\n        - forced numeric\n        - id (replacing NAs with -1)\n\n    Parameters:\n    df: raw data frame.\n    config: config dictionary as defined in get_config.\n    table_name: data source name, used for getting the parameters of the data set from the confing file..\n    columns_dict: used in case the user wants to override information in the config file.\n    check_column_existance: flag indicaing whether to tests that the columns that should be treated exist (throws an error if they don't).\n    verbose: flag indicating wether to print some statements.\n\n    Returns:\n    Cleaned dataset, with the parameters specified in config.\n\n    \"\"\"\n  \n    # TODO: add option to omit cleaning the df (to get the completely raw data)\n\n    # format column names\n    df.columns = format_as_character(df.columns)\n\n    # create dictionary with columns, unless specified in arguments\n    if config is not None:\n        columns_dict = get_columns_dict(config, table_name)\n\n    # if there is a replacement dictionary for names, apply it to the data frame\n    if columns_dict['new_names'] is not None:\n        # in this case tests are not necessary\n        df.rename(mapper=columns_dict['new_names'], axis='columns', inplace=True)\n\n    if columns_dict['to_drop'] is not None:\n        if check_column_existance:\n            # test that the columns to drop are in the data frame\n            test_columns_are_in_df(df, columns_dict['to_drop'])\n        else:\n            columns_dict['to_drop'] = keep_existing_cols(df, columns_dict['to_drop'])\n\n        # drop the columns specified in the json\n        df.drop(columns=columns_dict['to_drop'], inplace=True)\n\n    if columns_dict['na_replacements'] is not None:\n        # test that the columns defined in the previous dictionary exist\n        test_columns_are_in_df(df, list(columns_dict['na_replacements'].keys()))\n\n        # replace nas accordingly to previous dictionary\n        df.fillna(value=columns_dict['na_replacements'], inplace=True)\n\n    if columns_dict['to_keep'] is not None:\n        if len(columns_dict['to_keep']) > 0:\n            # make sure the columns specified in the dictionary are in the data frame - this test has to be run always\n            test_columns_are_in_df(df, columns_dict['to_keep'])\n            df = df[columns_dict['to_keep']]\n\n    if columns_dict['formats'] is not None:\n        if 'character' in columns_dict['formats'].keys():\n            for char_col in columns_dict['formats']['character']:\n                if char_col not in list(df.columns):\n                    print('Column {} has not been found in an instance of {}'.format(char_col, table_name))\n                else:\n                    if verbose:\n                        print('Formatting column {} from {} as character'.format(char_col, table_name))\n                    df.loc[:, char_col] = format_as_character(df[char_col])\n        if 'numeric' in columns_dict['formats'].keys():\n            for num_col in columns_dict['formats']['numeric']:\n                if num_col not in list(df.columns):\n                    print('Column {} has not been found in an instance of {}'.format(num_col, table_name))\n                else:\n                    if verbose:\n                        print('Formatting column {} from {} as numeric'.format(num_col, table_name))\n                    df.loc[:, num_col] = df[num_col].astype(float)\n        if 'dummify' in columns_dict['formats'].keys():\n            new_df = pd.get_dummies(df, dummy_na=True, columns=columns_dict['formats']['dummify'])\n            assert new_df.shape[0] == df.shape[0]\n            assert new_df.shape[1] > df.shape[1]\n            df = new_df\n            if verbose:\n                print('Columns {} have been dummified'.format(', '.join(columns_dict['formats']['dummify'])))\n        if 'forced_numeric' in columns_dict['formats'].keys():\n            for num_col in columns_dict['formats']['forced_numeric']:\n                if num_col not in list(df.columns):\n                    print('Column {} has not been found in an instance of {}'.format(num_col, table_name))\n                else:\n                    if verbose:\n                        print('Formatting forcebly column {} from {} as numeric'.format(num_col, table_name))\n                    df.loc[:, num_col] = pd.to_numeric(df[num_col], errors='coerce')\n        if 'id' in columns_dict['formats'].keys():\n            for id_col in columns_dict['formats']['id']:\n                if id_col not in list(df.columns):\n                    print('Column {} has not been found in an instance of {}'.format(num_col, table_name))\n                else:\n                    if verbose:\n                        print('Formatting forcebly column {} from {} as id'.format(num_col, table_name))\n                    df.loc[:, id_col] = pd.to_numeric(df[id_col], errors = 'coerce').fillna(-1).astype(int).astype(str)\n#                     df.loc[:, id_col] = df[id_col].fillna('-1').astype(float, errors = 'ignore').astype(int).astype(str).fillna('-1')\n                    pct_nas = (df[id_col] == '-1').mean()\n                    if pct_nas > 0:\n                        print('For table {} and column {}, the percentage of NAs is {}'.format(table_name, id_col, str(pct_nas)))\n\n    return df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"684c60b1-b221-4d9c-8ed4-09e77a0764c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def keep_existing_cols(df, cols):\n  \n    \"\"\"\n\n    Given a data frame and a list of column names, gets the columns that are in the data frame\n\n    Parameters:\n    df: data frame where the columns kept exist.\n    cols: list of column names.\n\n    Returns:\n    List of column names.\n\n    \"\"\"\n    \n    return list(set(cols).intersection(set(df.columns)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7231a3f3-5701-47e1-8710-6589bbc63ba6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_files_to_read(config, table_name):\n  \n    \"\"\"\n\n    Gives the files to read for reading a specific data source.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    table_name: name of the data source to read.\n\n    Returns:\n    List of full path file names.\n\n    \"\"\"\n  \n    files_to_read = list_files(config['paths']['raw'][table_name], full_path=True)\n    if len(files_to_read) == 1:\n        files_to_read = files_to_read[0]\n    return files_to_read"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27840947-8fa4-477f-bfeb-c87c2418576f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_and_clean_files(config, table_name, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans all files related to the specified data source\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    table_name: name of the data source to read.\n\n    Returns:\n    Data Frame consisting of all files appropiately treated and concatenated.\n\n    \"\"\"\n  \n    file_names = get_files_to_read(config, table_name)\n    output_df = pd.DataFrame({})\n    termination = file_names.split('.')[-1]\n    if (type(file_names) == str):\n        if termination == 'csv':\n            output_df = pd.read_csv(file_names, dtype = str)\n        else:\n            output_df = pd.read_excel(file_names)\n        output_df = clean_df(output_df, config, table_name, check_column_existance = False, verbose = verbose)\n    else:\n        for i in file_names:\n            print('Reading {} ...'.format(i))\n            if termination == 'csv':\n                temp_df = pd.read_csv(i, dtype = str)\n            else:\n                temp_df = pd.read_excel(i)\n            print('Cleaning {} ...'.format(i))\n            temp_df = clean_df(temp_df, config, table_name, check_column_existance = False, verbose = verbose)\n            output_df = pd.concat([output_df, temp_df], sort = False)\n\n    return output_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15696cb9-a82a-4f8c-a1b4-42fe0486512a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_df(config, table_name, verbose = False):\n    \n    \"\"\"\n\n    Wrapper for executing the get_raw function for the specified data set. The function name is read from the config dictionary.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    table_name: name of the data source to read.\n\n    Returns:\n    Data Frame consisting on the output of the get_raw function for the specified data set.\n\n    \"\"\"\n    \n    # make sure the table name exists in the config file\n    assert table_name in config['data_params'].keys(), 'Unknown table passed'\n    # extract function name\n    fun_name = config['data_params'][table_name]['functions']['raw']\n    # make sure the function is defined in the global environment\n    assert fun_name in globals().keys(), 'The function specified in the config file is not defined'\n    # exectute function to retrieve data\n    df = globals()[fun_name](config, verbose = verbose)\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a433ac50-7b0b-4398-ae07-4ad1518d7eb6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_tenant_history_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans tenant history data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned tenant history data.\n\n    \"\"\"\n    \n    raw_tenant_history = read_and_clean_files(config, 'tenant_history')\n    date_cols = [x for x in raw_tenant_history.columns if x.startswith('dt')]\n    for i in date_cols:\n        raw_tenant_history[i] = pd.to_datetime(raw_tenant_history[i])\n        raw_tenant_history[i] = pd.to_datetime(raw_tenant_history[i].dt.date)\n    return raw_tenant_history"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bff4f965-a072-4998-9f42-b6f39430f7f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_resi_detail_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans residential detail data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned residential detail data.\n\n    \"\"\"\n  \n  \n    resi_det = read_and_clean_files(config, 'residential_detail')\n    date_cols = [x for x in resi_det.columns if x.endswith('date')]\n    for i in date_cols:\n        resi_det[i] = pd.to_datetime(resi_det[i], errors = 'coerce')\n        resi_det[i] = pd.to_datetime(resi_det[i].dt.date)\n    return resi_det"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"123236e2-90a0-42cc-b855-2e6c799b1a5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_unitxref_data(config, verbose = False):\n  \n  # function not used for the model\n\n    unitxref = read_and_clean_files(config, 'unitxref')\n\n    date_cols = [x for x in unitxref.columns if x.startswith('dt')]\n    for i in date_cols:\n        unitxref[i] = pd.to_datetime(unitxref[i])\n        unitxref[i] = pd.to_datetime(unitxref[i].dt.date)\n\n    return unitxref"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d39d863-64bb-40eb-9f09-d6b577754440"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_prop_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans property data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned property data.\n\n    \"\"\"\n    \n    \n    prop = read_and_clean_files(config, 'property')\n\n    return prop\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8108468e-44c4-4586-a64b-15887d675805"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_unit_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans unit data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned unit data.\n\n    \"\"\"\n  \n    unit = read_and_clean_files(config, 'unit')\n\n    return unit"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfb3e89a-8a39-43e6-ab1a-d892ae09b34b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_unit_type_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans unit type data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned unit type data.\n\n    \"\"\"\n    \n    unit_type = read_and_clean_files(config, 'unit_type')\n\n    return unit_type    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"242b7f7d-8a7b-45d9-bf23-28e0a517c6a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\ndef get_raw_commamendments_data(config, verbose = False):\n  \n  # function not used for the model\n  \n    amme = read_and_clean_files(config, 'commamendments')\n    date_cols = [x for x in amme.columns if x.startswith('dt')]\n    for i in date_cols:\n        amme[i] = pd.to_datetime(amme[i])\n        amme[i] = pd.to_datetime(amme[i].dt.date)\n    return amme "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eb82f42-2b4f-4b54-9a30-4e05c3c9f6ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_coords_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans coordinates data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned coordinates data.\n\n    \"\"\"\n  \n    coords = read_and_clean_files(config, 'coords')\n    return coords    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e97dda22-196f-4896-829d-6ff97a1332a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_raw_sr_data(config, verbose = False):\n  \n    \"\"\"\n\n    Reads and cleans Service Requests data.\n\n    Parameters:\n    config: config dictionary as defined in get_config.\n    verbose: flag indicating whether to print certain messages.\n\n    Returns:\n    Cleaned Service Requests data.\n\n    \"\"\"\n  \n    sr = read_and_clean_files(config, 'service_requests')\n    date_cols = [x for x in sr.columns if x.startswith('dt')]\n    for i in date_cols:\n        sr[i] = pd.to_datetime(sr[i], errors = 'coerce')\n        sr[i] = pd.to_datetime(sr[i].dt.date, errors = 'coerce')\n    return sr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09576a21-fea7-4e77-87d6-2a71ee8810c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_ind_acc(config):\n  \n  \"\"\"\n\n  Gets array with individual account IDs (htenant).\n\n  Parameters:\n  config: config dictionary as defined in get_config.\n\n  Returns:\n  Array with individual account IDs (htenant).\n\n  \"\"\"\n\n  resi_detail = get_raw_df(config, 'residential_detail')\n  ind_acc = resi_detail.loc[resi_detail.type == 'individual', 'hcode'].values\n  return ind_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02478085-5978-4c5f-a112-4af91d42b73b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"get_raw_data","dashboards":[],"notebookMetadata":null,"language":"python","widgets":{},"notebookOrigID":1624358662289415}},"nbformat":4,"nbformat_minor":0}
