{"cells":[{"cell_type":"code","source":["def get_processed_srs(config):\n  \n  \"\"\"\n\n  Reads raw Service Requests data, and process it to make it ready for joining with the master table.\n\n  Parameters:\n  config: config dictionary as defined in get_config.\n\n  Returns:\n  Processed Service Requests data.\n\n  \"\"\"\n\n  sr = get_raw_df(config, 'service_requests')\n  sr['id_date'] = sr['dtupdatedt'] + pd.offsets.MonthBegin(1)\n  assert sr.id_date.isna().mean() < .01, 'There is more than 1% NAs in the update date of the Service Requests table'\n  sr = sr.loc[~sr.id_date.isna()]\n\n  id_dates_df = sr[['id_date']].drop_duplicates().assign(k = 1)\n  processed_srs = sr[['htenant', 'hunit']].drop_duplicates().assign(k = 1)\n  processed_srs = pd.merge(processed_srs, id_dates_df, on = 'k', how = 'outer').drop(columns = 'k')\n  processed_srs.sort_values(['htenant', 'hunit', 'id_date'], inplace = True)\n  assert processed_srs.isna().sum().sum() == 0\n\n  id_cols = ['htenant', 'hunit', 'id_date']\n  id_df = sr[id_cols].drop_duplicates()\n  agg_df = sr.groupby(id_cols)['hmy'].count().reset_index().rename(columns = {'hmy': 'n_SRs'})\n  id_df = pd.merge(id_df, agg_df, on = id_cols, how = 'left')\n  id_df['n_SRs'].fillna(0, inplace = True)\n\n\n  categories = {\n  'ssubcat': {\n    'term': 'termination_enquiry',\n    'change': 'change_of_unit',\n    'cfa': 'common_facilities_amenities',\n    'noise': 'complaint_noise'\n  },\n  'scategory': {\n    'ex_lease': 'existing_leasing_enquiry',\n    'common_area': 'pm_common_area',\n    'enq_lease': 'leasing_enquiry'\n  },\n  'spriority': {\n    'immediate': 'immediate',\n    'major': 'major'\n  }\n  }\n\n  def add_custom_count(id_df, sr, id_cols, categories, j, i):\n    new_col_name = 'n_SRs_' + j + '_' + i\n    agg_df = sr.loc[sr[j] == categories[j][i]].groupby(id_cols)['hmy'].count().reset_index().rename(columns = {'hmy': new_col_name})\n    id_df = pd.merge(id_df, agg_df, on = id_cols, how = 'left')\n    id_df[new_col_name].fillna(0, inplace = True)\n    return id_df\n\n  for j in categories.keys():\n    for i in categories[j].keys():\n      id_df = add_custom_count(id_df, sr, id_cols, categories, j, i)\n\n\n  processed_srs = pd.merge(processed_srs, id_df, on = id_cols, how = 'left').fillna(0)\n\n  processed_srs['n_avg_SRs_3m'] = processed_srs.groupby(['htenant', 'hunit'])['n_SRs'].rolling(3).mean().values\n  processed_srs['n_avg_SRs_6m'] = processed_srs.groupby(['htenant', 'hunit'])['n_SRs'].rolling(6).mean().values\n\n  for j in categories.keys():\n    for i in categories[j].keys():\n      new_col_name = 'n_SRs_' + j + '_' + i\n      processed_srs[new_col_name + '_3m'] = processed_srs.groupby(['htenant', 'hunit'])[new_col_name].rolling(3).mean().values\n      processed_srs[new_col_name + '_6m'] = processed_srs.groupby(['htenant', 'hunit'])[new_col_name].rolling(6).mean().values\n\n  processed_srs.fillna(0, inplace = True)\n\n  return processed_srs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b7775d8-8a78-4cea-aa99-de69191ee7ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"get_processed_data","dashboards":[],"notebookMetadata":null,"language":"python","widgets":{},"notebookOrigID":1624358662289413}},"nbformat":4,"nbformat_minor":0}
