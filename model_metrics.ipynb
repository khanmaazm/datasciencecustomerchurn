{"cells":[{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\nimport shap"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5df22158-0cd7-4d09-9545-2aaabe4c818d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_binary_target_summary(df, target_column, scores_col, n_buckets = 10, create_copy = False, feats = None):\n  \n    \"\"\"Returns metrics of the model per decile (accuracy, uplift, rolling rent and average values of the variables)\n\n    Parameters:\n    mt: Master table, stored in a Pandas Data Frame. It must contain the target, the scores, and the features.\n    target_column: string indicating the name of the column containing the target variable.\n    scores_col: string indicating the name of the column containing the scores of the model.\n    n_buckets: Number of buckets to divide the data frame in (for deciles, set this value to 10 and for percentiles to 100).\n    create_copy: indicates whether to create a copy in memory of the Data Frame.\n    feats: If provided, computes the average values of this features for the score buckets (deciles, percentiles, ...).\n\n    Returns:\n    Tuple of a data frame containing model performance per decile, and another one containing average values of the variables per decile\n\n    \"\"\"\n    \n    if create_copy:\n        df = df.copy()\n    \n    total_obs = df.shape[0]\n    total_pos_obs = df[target_column].sum()\n    avg_target_occurrence = df[target_column].mean()\n    n_elements_per_group = round(total_obs / n_buckets)\n    rolling_rent = df[df[target_column].values].crent.sum()\n    df.sort_values(scores_col, ascending = False, inplace = True)\n    df.loc[:, 'ntile'] = np.cumsum([((i % n_elements_per_group) == 0) for i in range(total_obs)])\n    df.loc[df.ntile > n_buckets, 'ntile'] = n_buckets\n    \n    # number of observations per ntile\n    summary_per_ntile = df.groupby('ntile')['hunit'].count().reset_index()\n    summary_per_ntile.rename(columns = {'hunit': 'n_obs'}, inplace = True)\n    \n    # number of positive observations per ntile\n    pos_obs_per_ntile = df[df[target_column].values].groupby('ntile')['hunit'].count().reset_index()\n    pos_obs_per_ntile.rename(columns = {'hunit': 'n_pos_obs'}, inplace = True)\n    \n    summary_per_ntile = pd.merge(summary_per_ntile, pos_obs_per_ntile, on = 'ntile', how = 'left')\n    summary_per_ntile.n_pos_obs.fillna(0, inplace = True)\n    \n    # rolling rent per ntile\n    rolling_rent_per_ntile = df[df[target_column].values].groupby('ntile')['crent'].sum().reset_index()\n    rolling_rent_per_ntile.rename(columns = {'crent': 'rolling_rent'}, inplace = True)\n    rolling_rent_per_ntile['rolling_rent'] = rolling_rent_per_ntile['rolling_rent'] / rolling_rent\n    \n    summary_per_ntile = pd.merge(summary_per_ntile, rolling_rent_per_ntile, on = 'ntile', how = 'left')\n    summary_per_ntile.rolling_rent.fillna(0, inplace = True)    \n    \n    # standard modelling metrics\n    summary_per_ntile.loc[:, 'accuracy'] = summary_per_ntile.n_pos_obs / summary_per_ntile.n_obs\n    summary_per_ntile.loc[:, 'uplift'] = summary_per_ntile.accuracy / avg_target_occurrence\n    summary_per_ntile.loc[:, 'recall'] = np.cumsum(summary_per_ntile.n_pos_obs) / total_pos_obs\n    summary_per_ntile.loc[:, 'acc_accuracy'] = (np.cumsum(summary_per_ntile.n_pos_obs) / np.cumsum(summary_per_ntile.n_obs))\n    summary_per_ntile.loc[:, 'acc_uplift'] =  summary_per_ntile.acc_accuracy / avg_target_occurrence\n    summary_per_ntile.loc[:, 'acc_rolling_rent'] = (np.cumsum(summary_per_ntile.rolling_rent))\n    \n#     min_score_per_bucket = df.groupby('ntile')[scores_col].min().reset_index().rename(columns = {scores_col: 'min_score'})\n#     avg_score_per_bucket = df.groupby('ntile')[scores_col].mean().reset_index().rename(columns = {scores_col: 'avg_score'})\n#     max_score_per_bucket = df.groupby('ntile')[scores_col].max().reset_index().rename(columns = {scores_col: 'max_score'})\n    \n    \n#     summary_per_ntile = pd.merge(summary_per_ntile, min_score_per_bucket, on = 'ntile', how = 'left')\n#     summary_per_ntile = pd.merge(summary_per_ntile, avg_score_per_bucket, on = 'ntile', how = 'left')\n#     summary_per_ntile = pd.merge(summary_per_ntile, max_score_per_bucket, on = 'ntile', how = 'left')\n\n    if feats is not None:\n      avg_feats_per_ntile =  df.groupby('ntile')[feats].mean().reset_index()\n    else:\n      avg_feats_per_ntile = None\n    \n    \n    return summary_per_ntile, avg_feats_per_ntile"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c870a88-224e-4af6-81f5-6a28513950e3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_imp_df(feats, model):\n  \n  \"\"\"Creates data frame with the gain of each variable of the model. Please not that this function might not work as is for all models (e.g., lightgbm), due to the difference in gettting the feature importances.\n\n  Parameters:\n  feats: list containing the feature names.\n  model: model on which the gain is to be computed\n\n  Returns:\n  Data Frame with two columns: feature name and feature importance\n\n  \"\"\"\n    \n  imp_df = pd.DataFrame({\n      'name': feats,\n      'imp': model.feature_importances_\n  }).sort_values('imp', ascending = False)\n  imp_df.loc[:, 'imp'] = imp_df.imp / imp_df.imp.sum()\n  return imp_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35811e3c-34b2-4f2c-a8b0-c83f39958975"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_shap_df(model, X):\n  \n  \"\"\"Calculates shap values of the features for the observations passed\n\n  Parameters:\n  model: model on which the shaps are to be computed.\n  X: Pandas Data Frame, in the same format as the ones passed for model training / predicting. \n\n  Returns:\n  Data Frame with as many rows as X and one more column: it contains the shap for each variable plus the baseline value (the same for all observations).\n\n  \"\"\"  \n  \n  explainer = shap.TreeExplainer(model),\n  shap_values = explainer.shap_values(X)\n  shap_values_df = pd.DataFrame(shap_values[1], columns = ['shap_' + i for i in X.columns])\n  shap_values_df['baseline_value'] = explainer.expected_value[1]\n  return shap_values_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05393d6d-4d87-4f66-8a2c-991efb8209d9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def add_model_scores(mt, model, feats, target_col, X_train, X_dev, X_test, X_leads):\n  \n  \"\"\"Gets master table with scores added. It only scores observations in a set (train, dev, test or leads)\n\n  Parameters:\n  mt: Pandas Data Frame containing the master table, for all sets.\n  model: model on which the shaps are to be computed.\n  feats: list containing the feature names.\n  X_train: Pandas Data Frame containing the features and only the train set. It could also be computed inside the function.\n  X_dev: Pandas Data Frame containing the features and only the dev set. It could also be computed inside the function.\n  X_test: Pandas Data Frame containing the features and only the test set. It could also be computed inside the function.\n  X_leads: Pandas Data Frame containing the features and only the leads set. It could also be computed inside the function.\n\n  Returns:\n  Data Frame one row per observation included in train, dev, test or leads, with the ID columns, the target, the features, the set column and a scores column named preds.\n\n  \"\"\"  \n  \n  output_df = mt.loc[~mt.set.isna()][['htent', 'hunit', 'id_date',  target_col, 'set'] + feats].copy()\n  output_df.loc[output_df.set == 'train', 'preds'] = model.predict_proba(X_train)[:, 1]\n  output_df.loc[output_df.set == 'dev', 'preds'] = model.predict_proba(X_dev)[:, 1]\n  output_df.loc[output_df.set == 'test', 'preds'] = model.predict_proba(X_test)[:, 1]\n  output_df.loc[output_df.set == 'leads', 'preds'] = model.predict_proba(X_leads)[:, 1]\n  return output_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3d965b5-367d-433d-a455-c8c4174a249b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_performance_dict(mt_with_scores, target_col, feats, model):\n  \n  \"\"\"Evaluates the performance of the model, using get_binary_target_summary and get_imp_df.\n\n  Parameters:\n  mt_with_scores: Pandas Data Frame as returned by add_model_scores.\n  target_col: string indicating the name of the column containing the target variable.\n  feats: list containing the feature names.\n  model: model on which the shaps are to be computed.\n\n  Returns:\n  Dictionary with five elements, each of them being a data frame:\n  - Performance per decile (accuracy, uplift, recall, ...) for the traning set, as returned by get_binary_target_summary.\n  - Performance per decile (accuracy, uplift, recall, ...) for the dev set, as returned by get_binary_target_summary.\n  - Performance per decile (accuracy, uplift, recall, ...) for the test set, as returned by get_binary_target_summary.\n  - Data Frame with feature importances, as returned by get_imp_df.\n  - Data Frame with average feature values per decile, as returned by get_binary_target_summary.\n\n  \"\"\"  \n  performance_dict = {}\n  for i in ['train', 'dev', 'test']:\n    performance_dict[i], avg_feats = get_binary_target_summary(df = mt_with_scores.loc[mt.set == i], target_column = target_col, scores_col = 'preds', n_buckets = 10, feats = feats)\n    \n  performance_dict['top_vars'] = get_imp_df(feats, model)\n  performance_dict['avg_values_per_decile'] = avg_feats    \n  \n  return performance_dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"811a4ed7-fa92-4835-9a5b-0f1fc7891131"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_leads_df(mt_with_scores):\n  \"\"\"Gets the leads Data Frame\n\n  Parameters:\n  mt_with_scores: Pandas Data Frame as returned by add_model_scores.\n\n  Returns:\n  Pandas Data Frame, with all leads (customers scored on the leads date), containing the ID columns and the scores.\n\n  \"\"\"  \n  leads_df = mt_with_scores.loc[mt_with_scores.set == 'leads'][['htent', 'hunit', 'id_date', 'set', 'preds']]\n  return leads_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"018b4ffd-80a7-42ba-9efc-ebc415c342f0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"model_metrics","dashboards":[],"notebookMetadata":null,"language":"python","widgets":{},"notebookOrigID":1624358662289467}},"nbformat":4,"nbformat_minor":0}
